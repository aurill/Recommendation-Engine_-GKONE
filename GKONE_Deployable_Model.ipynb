{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing Module"
      ],
      "metadata": {
        "id": "jMFmCm4LXZI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Nla-msK6Vtww",
        "outputId": "4d8f3590-9941-4f6b-f77f-1c5696b5247c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2504ca56-3038-4fbb-887c-bfbb78b694e5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2504ca56-3038-4fbb-887c-bfbb78b694e5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Updated_GKONE_Synthetic_Dataset.csv to Updated_GKONE_Synthetic_Dataset.csv\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import mse\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense\n",
        "from google.colab import files\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(file_name):\n",
        "    # Load the dataset\n",
        "    gkone_data = pd.read_csv(file_name)\n",
        "    initial_count = gkone_data.shape[0]\n",
        "\n",
        "    # Remove duplicate entries\n",
        "    gkone_data.drop_duplicates(inplace=True)\n",
        "    final_count = gkone_data.shape[0]\n",
        "    duplicates_removed = initial_count - final_count\n",
        "    missing_values = gkone_data.isnull().sum().sum()\n",
        "    na_values = gkone_data.isna().sum().sum()\n",
        "\n",
        "    # Trim white spaces from column names\n",
        "    gkone_data.columns = gkone_data.columns.str.strip()\n",
        "\n",
        "    # Trim white spaces from string values in each column\n",
        "    for col in gkone_data.select_dtypes(['object']).columns:\n",
        "        gkone_data[col] = gkone_data[col].str.strip()\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Number of duplicates removed: {duplicates_removed}\")\n",
        "    print(f\"Total missing values: {missing_values}\")\n",
        "    print(f\"Total NA values: {na_values}\")\n",
        "\n",
        "    return gkone_data"
      ],
      "metadata": {
        "id": "2-MPJfNMpDZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Classify Age Groups by Percentile\n",
        "\n",
        "def classify_age_groups(data):\n",
        "    percentiles = np.percentile(data['Age'], [25, 50, 75])\n",
        "    def age_group(age):\n",
        "        if age <= percentiles[0]:\n",
        "            return '0-25th Percentile'\n",
        "        elif age <= percentiles[1]:\n",
        "            return '25th-50th Percentile'\n",
        "        elif age <= percentiles[2]:\n",
        "            return '50th-75th Percentile'\n",
        "        else:\n",
        "            return '75th-100th Percentile'\n",
        "    data['AgeGroup'] = data['Age'].apply(age_group)\n",
        "    return data"
      ],
      "metadata": {
        "id": "JtawEuvuWCap"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: One-Hot Encoding and Normalization\n",
        "\n",
        "def one_hot_encode_and_normalize(data):\n",
        "    # One-hot encode age groups\n",
        "    age_group_dummies = pd.get_dummies(data['AgeGroup'], prefix='AgeGroup').astype(int)\n",
        "    data = pd.concat([data, age_group_dummies], axis=1)\n",
        "    data.drop(columns=['AgeGroup', 'Age'], inplace=True)\n",
        "\n",
        "    # One-hot encode product types\n",
        "    product_types = data['ProductType'].apply(lambda x: x.strip('[]').replace(\"'\", \"\").split(', '))\n",
        "    product_type_dummies = product_types.str.join('|').str.get_dummies()\n",
        "    data = pd.concat([data, product_type_dummies], axis=1)\n",
        "    data.drop(columns=['ProductType'], inplace=True)\n",
        "\n",
        "    # One-hot encode IncomeLevel column\n",
        "    income_level_encoded = pd.get_dummies(data['IncomeLevel'], prefix='IncomeLevel').astype(int)\n",
        "    data = pd.concat([data, income_level_encoded], axis=1)\n",
        "    data.drop(columns=['IncomeLevel'], inplace=True)\n",
        "\n",
        "    # Scale specified columns\n",
        "    columns_to_scale = [\n",
        "        'RemittancesFreq_Monthly', 'BillPaymentsFreq_Monthly', 'MarketPlaceFreq_Monthly',\n",
        "        'PeerToPeerFreq_Monthly', 'CustomerTenure', 'ActivityLevel'\n",
        "    ]\n",
        "    scaler = MinMaxScaler()\n",
        "    data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])\n",
        "\n",
        "    # One-hot encode categorical columns and convert to int\n",
        "    categorical_cols = ['Gender', 'EmploymentStatus', 'EducationLevel']\n",
        "    for col in categorical_cols:\n",
        "        if col in data.columns:\n",
        "            dummies = pd.get_dummies(data[col], prefix=col)\n",
        "            dummies = dummies.astype(int)  # Convert to int\n",
        "            data = pd.concat([data, dummies], axis=1)\n",
        "            data.drop(columns=[col], inplace=True)\n",
        "\n",
        "    # Standardize numerical columns\n",
        "    numerical_cols = ['Remittances_MonthlyTransValue', 'BillPayments_MonthlyTransValue', 'PeerToPeer_MonthlyTransValue', 'LinkedBankAccountMonthlyValue']\n",
        "    scaler = StandardScaler()\n",
        "    data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "    # Convert boolean to 0 and 1 for encoded columns\n",
        "    encoded_cols = [col for col in data.columns if data[col].dtype == 'bool']\n",
        "    data[encoded_cols] = data[encoded_cols].astype(int)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "QtDSV0tdtSo1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Location Classification\n",
        "\n",
        "top_parishes = {\n",
        "    'Remittances': ['Kingston', 'St. Andrew', 'St. Catherine'],\n",
        "    'MarketPlace': ['Kingston', 'St. Andrew', 'St. Catherine'],\n",
        "    'MotorInsurance': ['Kingston', 'St. Andrew', 'St. Catherine'],\n",
        "    'PeerToPeer Sending': ['Kingston', 'St. Andrew', 'St. Catherine'],\n",
        "    'BillPayments': ['St. James', 'St. Thomas', 'Westmoreland']\n",
        "}\n",
        "\n",
        "def count_categories_per_location(data, category):\n",
        "    # Filter the data for the specified category\n",
        "    category_data = data[data[category] == 1]\n",
        "\n",
        "    # Count the occurrences per location\n",
        "    location_counts = category_data['Location'].value_counts().sort_index()\n",
        "\n",
        "    return location_counts\n",
        "\n",
        "# Function to classify locations based on predefined top parishes and ranking\n",
        "def classify_location(data, top_parishes):\n",
        "    def classify_parish(parish, category, counts):\n",
        "        if parish in top_parishes[category]:\n",
        "            return 'Top'\n",
        "        sorted_counts = counts.sort_values(ascending=False)\n",
        "        mid_cutoff = int(len(sorted_counts) * 0.5)\n",
        "        if parish in sorted_counts.index[:mid_cutoff]:\n",
        "            return 'Mid'\n",
        "        return 'Low'\n",
        "\n",
        "    categories = list(top_parishes.keys())\n",
        "    location_category_counts = {}\n",
        "    for category in categories:\n",
        "        counts = count_categories_per_location(data, category)\n",
        "        classified_counts = counts.index.to_series().apply(lambda parish: classify_parish(parish, category, counts))\n",
        "        location_category_counts[category] = classified_counts\n",
        "\n",
        "    parish_classification_mapping = pd.DataFrame(index=data['Location'].unique())\n",
        "    for category in categories:\n",
        "        parish_classification_mapping[category] = parish_classification_mapping.index.map(location_category_counts[category].to_dict())\n",
        "\n",
        "    def determine_overall_classification(row):\n",
        "        if 'Top' in row.values:\n",
        "            return 'Top'\n",
        "        if 'Mid' in row.values:\n",
        "            return 'Mid'\n",
        "        return 'Low'\n",
        "\n",
        "    parish_classification_mapping['Overall_Classification'] = parish_classification_mapping.apply(determine_overall_classification, axis=1)\n",
        "    data['Overall_Classification'] = data['Location'].map(parish_classification_mapping['Overall_Classification'])\n",
        "\n",
        "    overall_classification_encoded = pd.get_dummies(data['Overall_Classification'], prefix='Usage').astype(int)\n",
        "    data = pd.concat([data, overall_classification_encoded], axis=1)\n",
        "    data.drop(columns=['Overall_Classification', 'Location'], inplace=True)\n",
        "\n",
        "    # Rename the columns\n",
        "    data.rename(columns={\n",
        "        'Usage_Top': 'High_Usage_Location',\n",
        "        'Usage_Mid': 'Moderate_Usage_Location',\n",
        "        'Usage_Low': 'Low_Usage_Location'\n",
        "    }, inplace=True)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "sGJrHpwdtAAw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Preprocess Data\n",
        "def preprocess_data(file_name):\n",
        "    gkone_data = load_and_preprocess_data(file_name)\n",
        "    gkone_data = classify_age_groups(gkone_data)\n",
        "    gkone_data = one_hot_encode_and_normalize(gkone_data)\n",
        "    gkone_data = classify_location(gkone_data, top_parishes)\n",
        "\n",
        "    # Drop unnecessary columns and convert data types if needed\n",
        "    if 'CustomerID' in gkone_data.columns:\n",
        "        gkone_data.drop(columns=['CustomerID'], inplace=True)\n",
        "    gkone_data = gkone_data.astype('float32')\n",
        "\n",
        "    # Save preprocessed data\n",
        "    save_preprocessed_data(gkone_data, 'gkone_preprocessed_data.pkl')\n",
        "\n",
        "    return gkone_data"
      ],
      "metadata": {
        "id": "4I4SsUIjtKRg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_preprocessed_data(data, file_name):\n",
        "    with open(file_name, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def load_preprocessed_data(file_name):\n",
        "    with open(file_name, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_name = '/content/Updated_GKONE_Synthetic_Dataset.csv'\n",
        "    preprocessed_data = preprocess_data(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2Sx1jNHttC95",
        "outputId": "cdc09bc9-78c4-4a8e-93e5-57944f2c03b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicates removed: 0\n",
            "Total missing values: 0\n",
            "Total NA values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building Module"
      ],
      "metadata": {
        "id": "HexTuGAbiOME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting seeds for reproducibility\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Ensuring TensorFlow uses a single thread (for reproducibility)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "# Define the input shape\n",
        "input_dim = preprocessed_data.shape[1]  # Number of features\n",
        "latent_dims = [2, 5, 10, 20]\n",
        "kl_weights = [0.5, 1.0, 2.0, 4.0]  # Different KL divergence weights to test\n",
        "\n",
        "best_reconstruction_loss = float('inf')\n",
        "best_kl_weight = None\n",
        "best_latent_dim = None\n",
        "best_vae = None\n",
        "\n",
        "for latent_dim in latent_dims:\n",
        "    for kl_weight in kl_weights:\n",
        "        # Define the VAE model with current latent_dim\n",
        "        inputs = Input(shape=(input_dim,))\n",
        "        h = Dense(64, activation='relu')(inputs)\n",
        "        h = Dense(32, activation='relu')(h)\n",
        "        z_mean = Dense(latent_dim)(h)\n",
        "        z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "        decoder_h1 = Dense(32, activation='relu')\n",
        "        decoder_h2 = Dense(64, activation='relu')\n",
        "        decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "        h_decoded = decoder_h1(z)\n",
        "        h_decoded = decoder_h2(h_decoded)\n",
        "        x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "        vae = Model(inputs, x_decoded_mean)\n",
        "\n",
        "        reconstruction_loss = mse(inputs, x_decoded_mean)\n",
        "        reconstruction_loss *= input_dim\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = K.sum(kl_loss, axis=-1)\n",
        "        kl_loss *= -0.5\n",
        "        vae_loss = K.mean(reconstruction_loss + kl_weight * kl_loss)  # Adjusted KL weight\n",
        "\n",
        "        vae.add_loss(vae_loss)\n",
        "        vae.compile(optimizer='adam')\n",
        "\n",
        "        # Train the VAE\n",
        "        vae.fit(preprocessed_data, preprocessed_data, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "        # Calculate reconstruction loss\n",
        "        recon_loss = vae.evaluate(preprocessed_data, preprocessed_data, verbose=0)\n",
        "\n",
        "        if recon_loss < best_reconstruction_loss:\n",
        "            best_reconstruction_loss = recon_loss\n",
        "            best_kl_weight = kl_weight\n",
        "            best_latent_dim = latent_dim\n",
        "            best_vae = vae\n",
        "\n",
        "# Print the best KL weight and latent dimension\n",
        "print(f'Best KL Weight: {best_kl_weight}, Best Latent Dimension: {best_latent_dim}')\n",
        "\n",
        "# Train the final VAE with the optimal latent_dim and KL weight\n",
        "inputs = Input(shape=(input_dim,))\n",
        "h = Dense(64, activation='relu')(inputs)\n",
        "h = Dense(32, activation='relu')(h)\n",
        "z_mean = Dense(best_latent_dim)(h)\n",
        "z_log_var = Dense(best_latent_dim)(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(best_latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "decoder_h1 = Dense(32, activation='relu')\n",
        "decoder_h2 = Dense(64, activation='relu')\n",
        "decoder_mean = Dense(input_dim, activation='sigmoid')\n",
        "h_decoded = decoder_h1(z)\n",
        "h_decoded = decoder_h2(h_decoded)\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "vae = Model(inputs, x_decoded_mean)\n",
        "\n",
        "reconstruction_loss = mse(inputs, x_decoded_mean)\n",
        "reconstruction_loss *= input_dim\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + best_kl_weight * kl_loss)  # Adjusted KL weight\n",
        "\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the final VAE\n",
        "vae.fit(preprocessed_data, preprocessed_data, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Encoder model to get the latent space\n",
        "encoder = Model(inputs, z_mean)\n",
        "\n",
        "# Getting the latent representations\n",
        "latent_representations = encoder.predict(preprocessed_data)\n",
        "\n",
        "# Save the latent representations to a file\n",
        "np.save('latent_representations.npy', latent_representations)\n",
        "\n",
        "# Save the VAE model\n",
        "vae.save('vae_model.h5')\n",
        "\n",
        "# Save the encoder model\n",
        "encoder.save('encoder_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv_84rZ8ikLp",
        "outputId": "2c733057-2c53-40cc-d239-99d7d17b5079"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KL Weight: 0.5, Best Latent Dimension: 5\n",
            "Epoch 1/50\n",
            "750/750 [==============================] - 5s 5ms/step - loss: 9.3066 - val_loss: 8.7543\n",
            "Epoch 2/50\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 8.3033 - val_loss: 8.1894\n",
            "Epoch 3/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 8.0531 - val_loss: 8.0668\n",
            "Epoch 4/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.9637 - val_loss: 7.9685\n",
            "Epoch 5/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.8937 - val_loss: 7.8880\n",
            "Epoch 6/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.8339 - val_loss: 7.8518\n",
            "Epoch 7/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.8048 - val_loss: 7.8206\n",
            "Epoch 8/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.7810 - val_loss: 7.8267\n",
            "Epoch 9/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.7664 - val_loss: 7.7796\n",
            "Epoch 10/50\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 7.7301 - val_loss: 7.7777\n",
            "Epoch 11/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 7.7313 - val_loss: 7.7743\n",
            "Epoch 12/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.7119 - val_loss: 7.7331\n",
            "Epoch 13/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.7117 - val_loss: 7.7116\n",
            "Epoch 14/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6896 - val_loss: 7.7278\n",
            "Epoch 15/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.6828 - val_loss: 7.7016\n",
            "Epoch 16/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 7.6761 - val_loss: 7.7007\n",
            "Epoch 17/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6703 - val_loss: 7.7028\n",
            "Epoch 18/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6656 - val_loss: 7.6906\n",
            "Epoch 19/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6574 - val_loss: 7.6753\n",
            "Epoch 20/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6479 - val_loss: 7.6657\n",
            "Epoch 21/50\n",
            "750/750 [==============================] - 4s 6ms/step - loss: 7.6444 - val_loss: 7.6802\n",
            "Epoch 22/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 7.6405 - val_loss: 7.6688\n",
            "Epoch 23/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6396 - val_loss: 7.6766\n",
            "Epoch 24/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6362 - val_loss: 7.7120\n",
            "Epoch 25/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6355 - val_loss: 7.6741\n",
            "Epoch 26/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 7.6323 - val_loss: 7.6866\n",
            "Epoch 27/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6276 - val_loss: 7.7073\n",
            "Epoch 28/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6098 - val_loss: 7.6802\n",
            "Epoch 29/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6260 - val_loss: 7.6632\n",
            "Epoch 30/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6158 - val_loss: 7.6868\n",
            "Epoch 31/50\n",
            "750/750 [==============================] - 3s 5ms/step - loss: 7.6203 - val_loss: 7.6526\n",
            "Epoch 32/50\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 7.6195 - val_loss: 7.6366\n",
            "Epoch 33/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6128 - val_loss: 7.6578\n",
            "Epoch 34/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6081 - val_loss: 7.6512\n",
            "Epoch 35/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.6085 - val_loss: 7.6388\n",
            "Epoch 36/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 7.6060 - val_loss: 7.6481\n",
            "Epoch 37/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.6069 - val_loss: 7.6309\n",
            "Epoch 38/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5967 - val_loss: 7.6142\n",
            "Epoch 39/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6054 - val_loss: 7.6210\n",
            "Epoch 40/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5935 - val_loss: 7.6614\n",
            "Epoch 41/50\n",
            "750/750 [==============================] - 4s 5ms/step - loss: 7.5935 - val_loss: 7.6312\n",
            "Epoch 42/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5928 - val_loss: 7.6384\n",
            "Epoch 43/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.6004 - val_loss: 7.6535\n",
            "Epoch 44/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5783 - val_loss: 7.6330\n",
            "Epoch 45/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5893 - val_loss: 7.6424\n",
            "Epoch 46/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.5827 - val_loss: 7.6061\n",
            "Epoch 47/50\n",
            "750/750 [==============================] - 3s 4ms/step - loss: 7.5876 - val_loss: 7.6269\n",
            "Epoch 48/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5851 - val_loss: 7.6319\n",
            "Epoch 49/50\n",
            "750/750 [==============================] - 2s 3ms/step - loss: 7.5917 - val_loss: 7.5972\n",
            "Epoch 50/50\n",
            "750/750 [==============================] - 3s 3ms/step - loss: 7.5870 - val_loss: 7.6088\n",
            "938/938 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load latent representations from the encoder\n",
        "latent_representations = encoder.predict(preprocessed_data)\n",
        "\n",
        "# Apply PCA to the Latent Representations\n",
        "pca = PCA(n_components=2)\n",
        "latent_representations_pca = pca.fit_transform(latent_representations)\n",
        "\n",
        "# Set the number of clusters to 4\n",
        "optimal_k = 4\n",
        "\n",
        "# Apply KMeans Clustering to the Latent Representations with the Optimal Number of Clusters\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(latent_representations)\n",
        "\n",
        "# Adding cluster labels to preprocessed data\n",
        "preprocessed_data['Cluster'] = cluster_labels\n",
        "\n",
        "# Save preprocessed data with cluster labels\n",
        "with open('/content/gkone_preprocessed_data.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessed_data, f)\n",
        "\n",
        "# Train the decision tree classifier\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree.fit(latent_representations, cluster_labels)\n",
        "\n",
        "# Save the models using joblib\n",
        "joblib.dump(decision_tree, 'decision_tree_model.pkl')\n",
        "joblib.dump(kmeans, 'kmeans_model.pkl')\n",
        "\n",
        "# Save the latent representations to a file\n",
        "np.save('latent_representations.npy', latent_representations)\n",
        "\n",
        "# Save the VAE model\n",
        "vae.save('vae_model.h5')\n",
        "\n",
        "# Save the encoder model\n",
        "encoder.save('encoder_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7shP_79gtbxB",
        "outputId": "dbb8dd4c-373f-468f-e700-b1815e3e7ff5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 2s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Evaluate the Decision Tree Classifier\n",
        "predicted_labels = decision_tree.predict(latent_representations)\n",
        "accuracy = accuracy_score(cluster_labels, predicted_labels)\n",
        "print(f\"Accuracy of Decision Tree Classifier on training data: {accuracy:.2f}\")\n",
        "\n",
        "# Cross-Validation to evaluate Decision Tree Classifier\n",
        "cv_scores = cross_val_score(decision_tree, latent_representations, cluster_labels, cv=5)\n",
        "mean_accuracy = np.mean(cv_scores)\n",
        "std_accuracy = np.std(cv_scores)\n",
        "print(f\"Cross-Validation Accuracy: {mean_accuracy:.2f} ± {std_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7N7c3Ez9zXH",
        "outputId": "eab032db-b26f-4360-ef2d-86eba2791cae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier on training data: 1.00\n",
            "Cross-Validation Accuracy: 0.99 ± 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction Module"
      ],
      "metadata": {
        "id": "F7aKw3McBULA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the pre-trained models and data\n",
        "decision_tree = joblib.load('decision_tree_model.pkl')\n",
        "kmeans = joblib.load('kmeans_model.pkl')\n",
        "vae = load_model('vae_model.h5')\n",
        "encoder = load_model('encoder_model.h5')\n",
        "\n",
        "latent_representations = np.load('latent_representations.npy')\n",
        "\n",
        "# Load preprocessed data for recommendation\n",
        "with open('/content/gkone_preprocessed_data.pkl', 'rb') as f:\n",
        "    preprocessed_data = pickle.load(f)\n",
        "\n",
        "# Preprocessing functions\n",
        "def preprocess_input(data):\n",
        "    # Drop the CustomerID column if present\n",
        "    if 'CustomerID' in data.columns:\n",
        "        data = data.drop(columns=['CustomerID'])\n",
        "    # Trim white spaces from column names\n",
        "    data.columns = data.columns.str.strip()\n",
        "    # Trim white spaces from string values in each column\n",
        "    for col in data.select_dtypes(['object']).columns:\n",
        "        data[col] = data[col].str.strip()\n",
        "    data = classify_age_groups(data)\n",
        "    data = one_hot_encode_and_normalize(data)\n",
        "    data = classify_location(data, top_parishes)\n",
        "    data = data.astype('float32')\n",
        "    return data\n",
        "\n",
        "# Function to classify new customer data\n",
        "def classify_new_customer(new_customer_data):\n",
        "    new_customer_data_preprocessed = preprocess_input(new_customer_data)\n",
        "    new_customer_latent = encoder.predict(new_customer_data_preprocessed)\n",
        "    cluster_label = decision_tree.predict(new_customer_latent)\n",
        "    return cluster_label, new_customer_latent\n",
        "\n",
        "# Function to recommend products/services based on cosine similarity with confidence scores\n",
        "def recommend_products_with_confidence(new_customer_vector, data, top_n=5):\n",
        "    product_columns = ['PeerToPeer Sending', 'MotorInsurance', 'Remittances', 'MarketPlace', 'BillPayments']\n",
        "\n",
        "    # Load the latent representations and preprocessed data\n",
        "    latent_representations = np.load('latent_representations.npy')\n",
        "    with open('/content/gkone_preprocessed_data.pkl', 'rb') as f:\n",
        "        preprocessed_data = pickle.load(f)\n",
        "\n",
        "    # Get the cluster label of the new customer\n",
        "    cluster_label = decision_tree.predict(new_customer_vector)\n",
        "\n",
        "    # Filter latent vectors to get those in the same cluster\n",
        "    same_cluster_indices = np.where(preprocessed_data['Cluster'] == cluster_label[0])[0]\n",
        "    same_cluster_latents = latent_representations[same_cluster_indices]\n",
        "\n",
        "    # Calculate cosine similarity between the new customer and customers in the same cluster\n",
        "    similarity_scores = cosine_similarity(new_customer_vector, same_cluster_latents)\n",
        "\n",
        "    # Get the top 5 most similar latent vectors\n",
        "    top_similar_indices = np.argsort(similarity_scores[0])[::-1][:top_n]\n",
        "    top_similar_customers = same_cluster_indices[top_similar_indices]\n",
        "\n",
        "    # Retrieve the original data rows corresponding to the top similar customers\n",
        "    top_similar_data = preprocessed_data.iloc[top_similar_customers]\n",
        "\n",
        "    # Exclude products already used by the new customer\n",
        "    new_customer_products = new_customer_vector[0][-len(product_columns):]\n",
        "    new_customer_products_used = [product_columns[i] for i in range(len(product_columns)) if new_customer_products[i] == 1]\n",
        "\n",
        "    # Calculate the weighted mean usage of each product among the top similar customers\n",
        "    similar_customers_products = top_similar_data[product_columns]\n",
        "    similar_customers_scores = similarity_scores[0][top_similar_indices]\n",
        "    weighted_usage = similar_customers_products.T.dot(similar_customers_scores)\n",
        "    weighted_usage /= similar_customers_scores.sum()\n",
        "\n",
        "    # Convert to percentages\n",
        "    confidence_scores = (weighted_usage / weighted_usage.sum()) * 100\n",
        "\n",
        "    # Exclude products already used by the new customer\n",
        "    recommended_products = confidence_scores.drop(new_customer_products_used)\n",
        "\n",
        "    # Sort recommendations by confidence scores\n",
        "    recommended_products = recommended_products.sort_values(ascending=False)\n",
        "\n",
        "    return recommended_products"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU1dWBeRE5xu",
        "outputId": "0070fa45-b5f7-49a8-bb93-e8c623399165"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function (Main.py)"
      ],
      "metadata": {
        "id": "8nm-LP5lFfP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load the original dataset to validate input\n",
        "    original_data = pd.read_csv('/content/Updated_GKONE_Synthetic_Dataset.csv')\n",
        "\n",
        "    valid_product_types = set(original_data['ProductType'].apply(lambda x: x.strip('[]').replace(\"'\", \"\").split(', ')).explode().unique())\n",
        "    valid_income_levels = set(original_data['IncomeLevel'].unique())\n",
        "    valid_locations = set(original_data['Location'].unique())\n",
        "    valid_genders = set(original_data['Gender'].unique())\n",
        "    valid_employment_statuses = set(original_data['EmploymentStatus'].unique())\n",
        "    valid_education_levels = set(original_data['EducationLevel'].unique())\n",
        "\n",
        "    # Prompt user for input\n",
        "    print(\"Please enter the following details for the new customer:\")\n",
        "\n",
        "    age = int(input(\"Age: \"))\n",
        "\n",
        "    product_types = validate_multiple_inputs(\"Product Types (e.g., PeerToPeer_Sending, MotorInsurance): \", valid_product_types)\n",
        "\n",
        "    income_level = validate_input(\"Income Level (Low/Medium/High): \", valid_income_levels)\n",
        "    remittances_freq = int(input(\"Remittances Frequency (Monthly): \"))\n",
        "    bill_payments_freq = int(input(\"Bill Payments Frequency (Monthly): \"))\n",
        "    marketplace_freq = int(input(\"MarketPlace Frequency (Monthly): \"))\n",
        "    peer_to_peer_freq = int(input(\"Peer-to-Peer Frequency (Monthly): \"))\n",
        "    customer_tenure = int(input(\"Customer Tenure (in years): \"))\n",
        "    activity_level = float(input(\"Activity Level (Number of Logins/ Month): \"))\n",
        "\n",
        "    location = validate_input(\"Location: \", valid_locations)\n",
        "    gender = validate_input(\"Gender (Male/Female): \", valid_genders)\n",
        "    employment_status = validate_input(\"Employment Status (Employed/Unemployed/Student/Retired): \", valid_employment_statuses)\n",
        "    education_level = validate_input(\"Education Level (High School/College/Graduate): \", valid_education_levels)\n",
        "    remittances_value = float(input(\"Remittances Monthly Transaction Value: \"))\n",
        "    bill_payments_value = float(input(\"Bill Payments Monthly Transaction Value: \"))\n",
        "    peer_to_peer_value = float(input(\"Peer-to-Peer Monthly Transaction Value: \"))\n",
        "    linked_bank_account_value = float(input(\"Linked Bank Account Monthly Value: \"))\n",
        "\n",
        "    # Create a DataFrame with the input data\n",
        "    new_customer_data = pd.DataFrame({\n",
        "        'Age': [age],\n",
        "        'ProductType': [product_types],\n",
        "        'IncomeLevel': [income_level],\n",
        "        'RemittancesFreq_Monthly': [remittances_freq],\n",
        "        'BillPaymentsFreq_Monthly': [bill_payments_freq],\n",
        "        'MarketPlaceFreq_Monthly': [marketplace_freq],\n",
        "        'PeerToPeerFreq_Monthly': [peer_to_peer_freq],\n",
        "        'CustomerTenure': [customer_tenure],\n",
        "        'ActivityLevel': [activity_level],\n",
        "        'Location': [location],\n",
        "        'Gender': [gender],\n",
        "        'EmploymentStatus': [employment_status],\n",
        "        'EducationLevel': [education_level],\n",
        "        'Remittances_MonthlyTransValue': [remittances_value],\n",
        "        'BillPayments_MonthlyTransValue': [bill_payments_value],\n",
        "        'PeerToPeer_MonthlyTransValue': [peer_to_peer_value],\n",
        "        'LinkedBankAccountMonthlyValue': [linked_bank_account_value]\n",
        "    })\n",
        "\n",
        "    # Combine the new customer data with the original dataset\n",
        "    combined_data = pd.concat([original_data, new_customer_data], ignore_index=True)\n",
        "\n",
        "    # Preprocess the combined data\n",
        "    preprocessed_combined_data = preprocess_input(combined_data)\n",
        "\n",
        "    # Extract the new customer data after preprocessing\n",
        "    new_customer_preprocessed = preprocessed_combined_data.iloc[-1:]\n",
        "\n",
        "    # Classify new customer and get latent representation\n",
        "    new_customer_latent = encoder.predict(new_customer_preprocessed)\n",
        "    cluster_label = decision_tree.predict(new_customer_latent)\n",
        "\n",
        "    # Manually assign the cluster label to the new customer data\n",
        "    new_customer_preprocessed['Cluster'] = cluster_label[0]\n",
        "\n",
        "    # Filter preprocessed data to include only customers from the same cluster\n",
        "    cluster_data = preprocessed_data[preprocessed_data['Cluster'] == cluster_label[0]]\n",
        "\n",
        "    # Recommend products/services for the new customer\n",
        "    recommendations = recommend_products_with_confidence(new_customer_latent, cluster_data)\n",
        "\n",
        "    print(\"Recommended Products with Confidence Scores:\")\n",
        "    print(recommendations)\n",
        "\n",
        "def validate_input(prompt, valid_options):\n",
        "    while True:\n",
        "        value = input(prompt).strip()\n",
        "        if value in valid_options:\n",
        "            return value\n",
        "        else:\n",
        "            print(f\"Invalid input. Valid options are: {valid_options}\")\n",
        "\n",
        "def validate_multiple_inputs(prompt, valid_options):\n",
        "    while True:\n",
        "        values = input(prompt).strip().split(',')\n",
        "        values = [value.strip() for value in values]\n",
        "        if all(value in valid_options for value in values):\n",
        "            return f\"['{', '.join(values)}']\"\n",
        "        else:\n",
        "            print(f\"Invalid input. Valid options are: {valid_options}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jn0oDI-lFe38",
        "outputId": "67a99498-ee3e-4faf-bc61-03a381a0a5f0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the following details for the new customer:\n",
            "Age: 32\n",
            "Product Types (e.g., PeerToPeer_Sending, MotorInsurance): Remittances, PeerToPeer Sending\n",
            "Income Level (Low/Medium/High): High\n",
            "Remittances Frequency (Monthly): 20\n",
            "Bill Payments Frequency (Monthly): 0\n",
            "MarketPlace Frequency (Monthly): 0\n",
            "Peer-to-Peer Frequency (Monthly): 17\n",
            "Customer Tenure (in years): 3\n",
            "Activity Level (Number of Logins/ Month): 15\n",
            "Location: St. James\n",
            "Gender (Male/Female): Female\n",
            "Employment Status (Employed/Unemployed/Student/Retired): Student\n",
            "Education Level (High School/College/Graduate): College\n",
            "Remittances Monthly Transaction Value: 29000.72\n",
            "Bill Payments Monthly Transaction Value: 0\n",
            "Peer-to-Peer Monthly Transaction Value: 17888.43\n",
            "Linked Bank Account Monthly Value: 13456.00\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "Recommended Products with Confidence Scores:\n",
            "Remittances           50.000000\n",
            "MarketPlace           29.910469\n",
            "MotorInsurance        20.089533\n",
            "PeerToPeer Sending     0.000000\n",
            "BillPayments           0.000000\n",
            "dtype: float32\n"
          ]
        }
      ]
    }
  ]
}